{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating a Raw Data Preprocessing Pipeline into ML and DL Pipelines\n",
    "\n",
    "This tutorial shows how to use a raw data preprocessing pipeline (for example, for ECG signals) and integrate it into both machine learning and deep learning workflows. We will:\n",
    "\n",
    "- Preprocess raw biosignals (using filtering and denoising functions)\n",
    "- Extract simple features from the preprocessed signals\n",
    "- Build a traditional machine learning pipeline (using scikit-learn) to classify the signals\n",
    "- Build a deep learning pipeline (using PyTorch) with a custom model and training loop\n",
    "\n",
    "Follow along to see how raw signal processing can be seamlessly integrated into subsequent model training and inference steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "We start by importing the required packages. In addition to standard libraries like NumPy, SciPy, and Matplotlib, we also import NeuroKit2 for signal simulation, PyTorch for deep learning, and scikit-learn for the machine learning classifier.\n",
    "\n",
    "Additionally, we import the repositoryâ€™s advanced ECG denoising function and configuration (note: parts of the raw signal pipeline are provided by the ECG_ACORAI repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the repository root folder (assuming the notebook is in /your-project-root/notebooks)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import neurokit2 as nk\n",
    "import torch\n",
    "\n",
    "# scikit-learn imports for ML pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import advanced ECG denoising from the repository (if available in your environment)\n",
    "from ecg_processor_torch.advanced_denoising import wavelet_denoise\n",
    "from ecg_processor_torch.config import ECGConfig\n",
    "\n",
    "# For inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete. ECG sampling rate:\", ECGConfig.DEFAULT_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulating and Preprocessing Raw Biosignals\n",
    "\n",
    "In a typical scenario, signals are acquired from sensors. For this tutorial, we'll simulate a noisy ECG signal using NeuroKit2. We then apply a preprocessing pipeline that includes filtering and (for ECG) an advanced wavelet denoising method.\n",
    "\n",
    "Below, we also define helper functions (for instance, Butterworth low-pass filtering) that can be reused for other signal types (e.g., EDA, EMG, Respiration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Butterworth low-pass filter and filter function\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def filter_signal(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Simulate a noisy ECG signal (10 seconds at fs Hz)\n",
    "fs = 500  # sampling frequency\n",
    "ecg_noisy = nk.ecg_simulate(duration=10, sampling_rate=fs, noise=0.1)\n",
    "\n",
    "# Plot the raw ECG signal\n",
    "t = np.linspace(0, 10, fs*10)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(t, ecg_noisy, label='Noisy ECG')\n",
    "plt.title('Simulated Noisy ECG Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Convert the ECG signal to a PyTorch tensor for the advanced denoising function\n",
    "ecg_tensor = torch.tensor(ecg_noisy, dtype=torch.float32)\n",
    "\n",
    "# Apply advanced wavelet denoising from the repository\n",
    "ecg_denoised_tensor = wavelet_denoise(ecg_tensor)\n",
    "\n",
    "# Convert the denoised signal back to NumPy for further processing and visualization\n",
    "ecg_denoised = ecg_denoised_tensor.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(t, ecg_noisy, label='Noisy ECG', alpha=0.6)\n",
    "plt.plot(t, ecg_denoised, label='Denoised ECG', linewidth=2)\n",
    "plt.title('Advanced ECG Denoising')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "After preprocessing the raw signals, we need to extract features that can be used for classification. In this example, we extract simple statistical features (mean, standard deviation, min, max) from each ECG signal. In actual applications, you might include frequency-domain, morphological, or other domain-specific features.\n",
    "\n",
    "The following cell demonstrates a simple feature extraction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(signal):\n",
    "    features = {}\n",
    "    features['mean'] = np.mean(signal)\n",
    "    features['std'] = np.std(signal)\n",
    "    features['min'] = np.min(signal)\n",
    "    features['max'] = np.max(signal)\n",
    "    return features\n",
    "\n",
    "# Extract features from the denoised ECG signal\n",
    "features = extract_features(ecg_denoised)\n",
    "print(\"Extracted features:\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integrating with a Machine Learning Pipeline\n",
    "\n",
    "In this section, we demonstrate how to use the extracted features in a traditional machine learning model. We simulate a simple classification task where each ECG signal is assigned a label (here, we use synthetic labels for demonstration). Then we split the data into training and testing sets, train a Random Forest classifier from scikit-learn, and evaluate its accuracy.\n",
    "\n",
    "In practice, you would extract features from many signals to create a feature matrix and label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, create synthetic data\n",
    "\n",
    "# Let's assume we have 100 samples (signals) and we extract the same 4 features from each\n",
    "num_samples = 100\n",
    "X = []\n",
    "y = []\n",
    "for i in range(num_samples):\n",
    "    # Simulate a noisy ECG signal\n",
    "    ecg_sample = nk.ecg_simulate(duration=10, sampling_rate=fs, noise=0.1)\n",
    "    ecg_tensor_sample = torch.tensor(ecg_sample, dtype=torch.float32)\n",
    "    ecg_denoised_sample = wavelet_denoise(ecg_tensor_sample).cpu().numpy()\n",
    "    feats = extract_features(ecg_denoised_sample)\n",
    "    # Create a feature vector\n",
    "    feat_vector = [feats['mean'], feats['std'], feats['min'], feats['max']]\n",
    "    X.append(feat_vector)\n",
    "    \n",
    "    # Simulate binary labels (0 or 1) for demonstration\n",
    "    label = 0 if np.mean(ecg_denoised_sample) < 0 else 1\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Random Forest Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integrating with a Deep Learning Pipeline (Using PyTorch)\n",
    "\n",
    "For deep learning, we can build an end-to-end pipeline that accepts raw or preprocessed signals, extracts features, and feeds them to a neural network. In this example, we create a simple feed-forward network that uses the same feature vector as the ML pipeline. We define a PyTorch `Dataset` and `DataLoader`, build a simple model, run a training loop, and evaluate the performance.\n",
    "\n",
    "This should give you a template that you can extend to more sophisticated deep learning architectures (like CNNs, RNNs, or transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a PyTorch Dataset for our feature vectors\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ECGDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define a simple feed-forward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "input_dim = 4   # four features per sample\n",
    "hidden_dim = 16\n",
    "num_classes = 2\n",
    "model = SimpleNN(input_dim, hidden_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(\"Deep Learning Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this tutorial we integrated a raw data preprocessing pipeline into both a traditional machine learning pipeline and a deep learning pipeline. We:\n",
    "\n",
    "- Simulated and preprocessed raw ECG signals\n",
    "- Extracted simple statistical features\n",
    "- Built a Random Forest classifier using scikit-learn\n",
    "- Built a simple PyTorch-based neural network for classification\n",
    "\n",
    "This comprehensive integration demonstrates how you can tie together signal preprocessing, feature extraction, and different model training frameworks into a seamless workflow. In real projects, you may replace the simple feature extraction method with complex domain-specific routines and replace the simple models with more sophisticated architectures.\n",
    "\n",
    "Happy coding and model building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
